{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import strftime\n",
    "import datetime\n",
    "import time\n",
    "from pandas import ExcelWriter\n",
    "import re\n",
    "import pyodbc\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "import json\n",
    "from sc_header import createDriver, agregar_links , scrape_url, scrape_batch,aplanar_lista, check_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha = datetime.date.today()\n",
    "hoy = fecha.strftime('%Y/%m/%d')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser.delete_all_cookies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = createDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.delete_all_cookies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_sin_color = [['https://www.shein.com/Jeans-c-1934.html?icn=jeans&ici=www_tab01navbar04menu21dir02',\n",
    "                    'JEAN'],\n",
    "                   ['https://www.shein.com/Denim-Jackets-c-1933.html?icn=denim-jackets&ici=www_tab01navbar04menu21dir01',\n",
    "                    'CHAQUETA DE JEAN'],\n",
    "                   ['https://www.shein.com/Denim-Shorts-c-1935.html?icn=denim-shorts&ici=www_tab01navbar04menu21dir03',\n",
    "                   'SHORT DE JEAN'],\n",
    "                   ['https://www.shein.com/Leggings-c-1871.html?icn=leggings&ici=www_tab01navbar04menu12dir02',\n",
    "                   'CALZA'],\n",
    "                   ['https://www.shein.com/Active-Tops-c-2181.html?icn=active-tops&ici=www_tab01navbar04menu22dir02',\n",
    "                   'TOP DEPORTIVO'],\n",
    "                   ['https://www.shein.com/Skirts-c-1732.html?icn=skirts&ici=www_tab01navbar04menu12dir03',\n",
    "                   'FALDA'],\n",
    "                   ['https://www.shein.com/Shorts-c-1912.html?icn=shorts&ici=www_tab01navbar04menu12dir04',\n",
    "                   'SHORT'],\n",
    "                   ['https://www.shein.com/Pants-c-1740.html?icn=pants&ici=www_tab01navbar04menu12dir01',\n",
    "                   'PANTALON']\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://www.shein.com'\n",
    "Tlistas = []\n",
    "for lsc in links_sin_color:\n",
    "    \n",
    "    agregar_links(lsc[0],\n",
    "                 browser,\n",
    "                 Tlistas,\n",
    "                 lsc[1],\n",
    "                 url_base)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CALZA',\n",
       " 'CHAQUETA DE JEAN',\n",
       " 'FALDA',\n",
       " 'JEAN',\n",
       " 'PANTALON',\n",
       " 'SHORT',\n",
       " 'SHORT DE JEAN',\n",
       " 'TOP DEPORTIVO'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([item[0] for item in Tlistas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Army Green',\n",
       " 'Beige',\n",
       " 'Black',\n",
       " 'Black and White',\n",
       " 'Blue',\n",
       " 'Bright',\n",
       " 'Brown',\n",
       " 'Burgundy',\n",
       " 'Camel',\n",
       " 'Gold',\n",
       " 'Green',\n",
       " 'Grey',\n",
       " 'Khaki',\n",
       " 'Multicolor',\n",
       " 'Navy',\n",
       " 'Nude',\n",
       " 'Ombre',\n",
       " 'Orange',\n",
       " 'Pastel',\n",
       " 'Pink',\n",
       " 'Purple',\n",
       " 'Red',\n",
       " 'White',\n",
       " 'Yellow'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([item[1] for item in Tlistas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser = createDriver()\n",
    "hrefs_list = []\n",
    "ex_col = []\n",
    "for lista in Tlistas:\n",
    "    try:\n",
    "        browser.get(lista[2])\n",
    "    except:\n",
    "        browser.quit()\n",
    "        time.sleep(1)\n",
    "        browser = createDriver()\n",
    "        browser.get(lista[2])\n",
    "    \n",
    "    while True:\n",
    "        soup = BeautifulSoup(browser.page_source,'html.parser')\n",
    "        \n",
    "        indice_=0\n",
    "        for element in soup.select('div.j-switch-color-wrap > div.c-goodsitem__ratiowrap > a'):\n",
    "        #for element in soup.find_all(class_='c-goodsitem__ratiowrap'):\n",
    "            try:\n",
    "                \n",
    "                href_aux = url_base +'/'+ element['href']\n",
    "                \n",
    "                hrefs_list.append([indice_,\n",
    "                                   lista[0],\n",
    "                                   lista[1],\n",
    "                                   lista[2],\n",
    "                                   href_aux\n",
    "                                  ])\n",
    "                indice_+=1\n",
    "                \n",
    "            except:\n",
    "                ex_col.append([lista[2],lista[1]])\n",
    "                pass            \n",
    "        try:\n",
    "            browser.find_element_by_class_name('page-next').find_element_by_tag_name('a').click()\n",
    "        except:\n",
    "            break\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hrefs_list,columns=['pos','tipo','color','url_scraper','href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['auxi'] = df['href'].apply(check_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4932, 4932)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df),len(df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df['tipo'].unique().tolist()) != len(set([item[0] for item in Tlistas])):\n",
    "    print('REVISAR LOS TIPOS SCRAPEADOS')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ = datetime.datetime.now()\n",
    "items = []\n",
    "errores = []\n",
    "\n",
    "batch_size = 5\n",
    "url_chunks = [df[x:x+batch_size] for x in range(0, len(df), batch_size)]\n",
    "\n",
    "var = 0\n",
    "for ix, url_chunk in enumerate(url_chunks):\n",
    "    scrape_batch(errores,items,url_chunk)\n",
    "    \n",
    "    if int(ix/50) == ix/50:\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(items)\n",
    "            df.to_excel('jeans.xlsx')\n",
    "            \n",
    "            if len(errores) >0:\n",
    "                \n",
    "                pd.DataFrame(errores).to_excel('jeans_error.xlsx')\n",
    "                \n",
    "            file = open(\"jeans.txt\", \"w\")\n",
    "            file.write( str(url_chunk) + os.linesep)\n",
    "            file.close()\n",
    "        except:\n",
    "            print(\"Erro al crear .xlsx\")    \n",
    "    \n",
    "end_ = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_ = datetime.datetime.now()\n",
    "# items = []\n",
    "\n",
    "# batch_size = 5\n",
    "# url_chunks = [df[x:x+batch_size] for x in range(0, len(df), batch_size)]\n",
    "\n",
    "# var = 0\n",
    "# for url_chunk in url_chunks:\n",
    "#     items.append(scrape_batch(url_chunk))\n",
    "#     var += 1\n",
    "    \n",
    "#     if var == 50:\n",
    "#         var = 0\n",
    "#         try:\n",
    "#             df = pd.DataFrame(items)\n",
    "#             df.to_excel('jeans.xlsx')\n",
    "                                            \n",
    "#             file = open(\"jeans.txt\", \"w\")\n",
    "#             file.write( str(url_chunk) + os.linesep)\n",
    "#             file.close()\n",
    "#         except:\n",
    "#             print(\"Erro al crear .xlsx\")\n",
    "        \n",
    "    \n",
    "# end_ = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##COPIADOOO\n",
    "# new_df = df[(df['tipo'] != 'JEAN') &\n",
    "#             (df['tipo'] != 'CHAQUETA DE JEAN') &\n",
    "#             (df['tipo'] != 'SHORT DE JEAN') &\n",
    "#             (df['tipo'] != 'CALZA')]\n",
    "\n",
    "# batch_size = 5\n",
    "# url_chunks = [new_df[x:x+batch_size] for x in range(0, len(new_df), batch_size)]\n",
    "\n",
    "# var = 0\n",
    "# for url_chunk in url_chunks:\n",
    "#     items.append(scrape_batch(url_chunk))\n",
    "#     var += 1\n",
    "    \n",
    "#     if var == 50:\n",
    "#         var = 0\n",
    "#         try:\n",
    "#             df = pd.DataFrame(items)\n",
    "#             df.to_excel('jeans.xlsx')\n",
    "                                            \n",
    "#             file = open(\"jeans.txt\", \"w\")\n",
    "#             file.write( str(url_chunk) + os.linesep)\n",
    "#             file.close()\n",
    "#         except:\n",
    "#             print(\"Erro al crear .xlsx\")\n",
    "        \n",
    "    \n",
    "# end_ = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 2:12:22.23\n"
     ]
    }
   ],
   "source": [
    "print('Tiempo de ejecución: {}'.format(end_ - start_)[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = aplanar_lista(items)\n",
    "new_list = [i for i in new_list if type(i) != int]\n",
    "dfShein = pd.DataFrame(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShein = dfShein.rename(columns={0: 'pos'})\n",
    "dfShein = dfShein.rename(columns={1: 'Tipo'})\n",
    "dfShein = dfShein.rename(columns={2: 'Color'})\n",
    "dfShein = dfShein.rename(columns={3: 'id Producto'})\n",
    "dfShein = dfShein.rename(columns={4: 'Descripcion'})\n",
    "dfShein = dfShein.rename(columns={5: 'Talle'})\n",
    "dfShein = dfShein.rename(columns={6: 'Stock'})\n",
    "dfShein = dfShein.rename(columns={7: 'Precio'})\n",
    "dfShein = dfShein.rename(columns={8: 'Precio_dto'})\n",
    "dfShein = dfShein.rename(columns={9: 'Url Imagen'})\n",
    "dfShein = dfShein.rename(columns={10: 'Url Producto'})\n",
    "dfShein = dfShein.rename(columns={11: 'pagina_scraper'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShein[\"Fecha\"] = hoy\n",
    "dfShein[\"Marca\"] = \"SHEIN\"\n",
    "dfShein[\"Moneda\"] = \"DOLAR US$\"\n",
    "dfShein['Sexo'] = 'Mujer'\n",
    "dfShein['Descripcion aux'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginas = len(dfShein['pagina_scraper'].unique())\n",
    "\n",
    "file = open(\"jeans.txt\", \"w\")\n",
    "file.write(  os.linesep +f'CANTIDAD DE ITEMS - {len(dfShein)}' ) \n",
    "file.write(  os.linesep +f'DURACION - {format(end_ - start_)[:-4]}' ) \n",
    "file.write(  os.linesep +f'CANTIDAD DE HOJAS - {paginas}' ) \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(dfShein['Tipo'].unique().tolist()) != len(set([item[0] for item in Tlistas])):\n",
    "    print('REVISAR LOS TIPOS SCRAPEADOS')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShein.to_excel(f'../Salida/dfShein_jeans{fecha}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19294, 19294, 19294)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfShein.drop_duplicates()),len(dfShein),len(dfShein.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POR SI SE QUEDA CORRER ESTO\n",
    "# startt_ = datetime.datetime.now()\n",
    "# var = 0\n",
    "# for url_chunk in copy_chunks:\n",
    "#     items.append(scrape_batch(url_chunk))\n",
    "#     var += 1\n",
    "    \n",
    "#     if var == 50:\n",
    "#         var = 0\n",
    "#         try:\n",
    "#             df = pd.DataFrame(items)\n",
    "#             df.to_excel('jeans.xlsx')\n",
    "                                            \n",
    "#             file = open(\"jeans.txt\", \"w\")\n",
    "#             file.write( str(url_chunk) + os.linesep)\n",
    "#             file.close()\n",
    "#         except:\n",
    "#             print(\"Erro al crear .xlsx\")\n",
    "    \n",
    "# end_ = datetime.datetime.now()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
