{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import strftime\n",
    "import datetime\n",
    "import time\n",
    "from pandas import ExcelWriter\n",
    "import re\n",
    "import pyodbc\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "import selenium\n",
    "import itertools\n",
    "import json\n",
    "from sc_header import createDriver, agregar_links, scrape_url, scrape_batch, aplanar_lista, check_length,aplanar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha = datetime.date.today()\n",
    "hoy = fecha.strftime('%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = createDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser.get('https://www.shein.com/Plus-Size-c-1888.html?icn=plus-size&ici=www_tab01navbar05menu01&scici=navbar_2~~tab01navbar05menu01~~5_1~~real_1888~~SPcCccWomenCategory_default~~0~~0')\n",
    "#try:\n",
    "#    browser.execute_script('document.querySelector(\"body > div.welcome-privacy.j-welcome-privacy > div > div.c-modal-wrap.welocme-modal > div.c-modal > div > div > div.modal-body > div > div > button\").click()')\n",
    "#except:\n",
    "#    pass\n",
    "#page = BeautifulSoup(browser.page_source,'html.parser')\n",
    "url_base = 'https://www.shein.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser.delete_all_cookies()\n",
    "links_sin_color =[['https://www.shein.com/Plus-Size-c-1888.html?icn=plus-size&ici=www_tab01navbar05menu01&scici=navbar_2~~tab01navbar05menu01~~5_1~~real_1888~~SPcCccWomenCategory_default~~0~~0',\n",
    "                 'PLUS SIZE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tlistas = []\n",
    "for lsc in links_sin_color:\n",
    "    \n",
    "    agregar_links(lsc[0],\n",
    "                 browser,\n",
    "                 Tlistas,\n",
    "                 lsc[1],\n",
    "                 url_base)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser = createDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COPIADO PORQUE PARECE QUE NO FUNCA\n",
    "# hrefs_list = []\n",
    "# ex_col = []\n",
    "# for lista in Tlistas:\n",
    "#     browser.get(lista[2])\n",
    "    \n",
    "#     while True:\n",
    "        \n",
    "#         elements_goodsitem = browser.find_elements_by_class_name('c-goodsitem__ratiowrap')\n",
    "#         indice_ = 0\n",
    "#         for element in elements_goodsitem:\n",
    "#             try:\n",
    "#                 href_aux = element.find_element_by_tag_name('a').get_attribute('href')\n",
    "#                 hrefs_list.append([indice_,\n",
    "#                                    lista[0],\n",
    "#                                    lista[1],\n",
    "#                                    lista[2],\n",
    "#                                    href_aux\n",
    "#                                   ])\n",
    "                \n",
    "#                 indice_+=1\n",
    "#             except:\n",
    "#                 ex_col.append([lista[2],lista[1]])\n",
    "#                 pass\n",
    "            \n",
    "#         try:\n",
    "#             browser.find_element_by_class_name('page-next').find_element_by_tag_name('a').click()\n",
    "#         except:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs_list = []\n",
    "ex_col = []\n",
    "for lista in Tlistas:\n",
    "    try:\n",
    "        browser.get(lista[2])\n",
    "    except:\n",
    "        browser.quit()\n",
    "        time.sleep(1)\n",
    "        browser = createDriver()\n",
    "        browser.get(lista[2])\n",
    "    \n",
    "    while True:\n",
    "        soup = BeautifulSoup(browser.page_source,'html.parser')\n",
    "        \n",
    "        indice_=0\n",
    "        for element in soup.select('div.j-switch-color-wrap > div.c-goodsitem__ratiowrap > a'):\n",
    "        #for element in soup.find_all(class_='c-goodsitem__ratiowrap'):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                href_aux = url_base +'/'+ element['href']\n",
    "                \n",
    "                hrefs_list.append([indice_,\n",
    "                                   lista[0],\n",
    "                                   lista[1],\n",
    "                                   lista[2],\n",
    "                                   href_aux\n",
    "                                  ])\n",
    "                indice_+=1\n",
    "                \n",
    "            except:\n",
    "                ex_col.append([lista[2],lista[1]])\n",
    "                pass            \n",
    "        try:\n",
    "            browser.find_element_by_class_name('page-next').find_element_by_tag_name('a').click()\n",
    "        except:\n",
    "            break\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hrefs_list,columns=['pos','tipo','color','url_scraper','href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['auxi'] = df['href'].apply(check_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8735, 8735)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.drop_duplicates()),len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df['tipo'].unique().tolist()) != len(set([item[0] for item in Tlistas])):\n",
    "    print('REVISAR LOS TIPOS SCRAPEADOS')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_ = datetime.datetime.now()\n",
    "# items = []\n",
    "# errores = []\n",
    "\n",
    "# batch_size = 5\n",
    "# url_chunks = [df[x:x+batch_size] for x in range(0, len(df), batch_size)]\n",
    "\n",
    "# var = 0\n",
    "# for ix, url_chunk in enumerate(url_chunks):\n",
    "#     scrape_batch(url_chunk)\n",
    "    \n",
    "#     if int(ix/50) == ix/50:\n",
    "        \n",
    "#         try:\n",
    "#             df = pd.DataFrame(items)\n",
    "#             df.to_excel('plus.xlsx')\n",
    "            \n",
    "#             if len(errores) >0:\n",
    "                \n",
    "#                 pd.DataFrame(errores).to_excel('plus_error.xlsx')\n",
    "                \n",
    "#             file = open(\"plus.txt\", \"w\")\n",
    "#             file.write( str(url_chunk) + os.linesep)\n",
    "#             file.close()\n",
    "#         except:\n",
    "#             print(\"Erro al crear .xlsx\")    \n",
    "    \n",
    "# end_ = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ = datetime.datetime.now()\n",
    "items = []\n",
    "errores = []\n",
    "\n",
    "batch_size = 5\n",
    "url_chunks = [df[x:x+batch_size] for x in range(0, len(df), batch_size)]\n",
    "\n",
    "var = 0\n",
    "for ix, url_chunk in enumerate(url_chunks):\n",
    "    scrape_batch(errores,items,url_chunk)\n",
    "    \n",
    "    if int(ix/50) == ix/50:\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(items)\n",
    "            df.to_excel('plus.xlsx')\n",
    "            \n",
    "            if len(errores) >0:\n",
    "                \n",
    "                pd.DataFrame(errores).to_excel('plus_error.xlsx')\n",
    "                \n",
    "            file = open(\"plus.txt\", \"w\")\n",
    "            file.write( str(url_chunk) + os.linesep)\n",
    "            file.close()\n",
    "        except:\n",
    "            print(\"Erro al crear .xlsx\")    \n",
    "    \n",
    "end_ = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #COPIADO PORQUE FALLO\n",
    "# start_ = datetime.datetime.now()\n",
    "# batch_size = 5\n",
    "# url_chunks = [df[x:x+batch_size] for x in range(0, len(df), batch_size)]\n",
    "\n",
    "# var = 0\n",
    "# for url_chunk in url_chunks:\n",
    "#     items.append(scrape_batch(url_chunk))\n",
    "#     var += 1\n",
    "    \n",
    "#     if var == 50:\n",
    "#         var = 0\n",
    "#         try:\n",
    "#             df = pd.DataFrame(items)\n",
    "#             df.to_excel('plus_size.xlsx')\n",
    "            \n",
    "                                                                                \n",
    "#             file = open(\"plus_size.txt\", \"w\")\n",
    "#             file.write( str(url_chunk) + os.linesep)\n",
    "#             file.close()\n",
    "#         except:\n",
    "#             print(\"Erro al crear .xlsx\")\n",
    "    \n",
    "# end_ = datetime.datetime.now()\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 2:07:43.93\n"
     ]
    }
   ],
   "source": [
    "print('Tiempo de ejecución: {}'.format(end_ - start_)[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVISAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_list = aplanar_lista(items)\n",
    "#new_list = [i for i in new_list if type(i) != int]\n",
    "dfShein = pd.DataFrame(items)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = aplanar(items)\n",
    "# a =[i for i in a if type(i) != int]\n",
    "# b = aplanar(a)\n",
    "# dfShein = pd.DataFrame(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShein = dfShein.rename(columns={0: 'pos'})\n",
    "dfShein = dfShein.rename(columns={1: 'Tipo'})\n",
    "dfShein = dfShein.rename(columns={2: 'Color'})\n",
    "dfShein = dfShein.rename(columns={3: 'id Producto'})\n",
    "dfShein = dfShein.rename(columns={4: 'Descripcion'})\n",
    "dfShein = dfShein.rename(columns={5: 'Talle'})\n",
    "dfShein = dfShein.rename(columns={6: 'Stock'})\n",
    "dfShein = dfShein.rename(columns={7: 'Precio'})\n",
    "dfShein = dfShein.rename(columns={8: 'Precio_dto'})\n",
    "dfShein = dfShein.rename(columns={9: 'Url Imagen'})\n",
    "dfShein = dfShein.rename(columns={10: 'Url Producto'})\n",
    "dfShein = dfShein.rename(columns={11: 'pagina_scraper'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShein[\"Fecha\"] = hoy\n",
    "dfShein[\"Marca\"] = \"SHEIN\"\n",
    "dfShein[\"Moneda\"] = \"DOLAR US$\"\n",
    "dfShein['Sexo'] = 'Mujer'\n",
    "dfShein['Descripcion aux'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginas = len(dfShein['pagina_scraper'].unique())\n",
    "file = open(\"plus_size.txt\", \"a\")\n",
    "file.write(  os.linesep +f'CANTIDAD_ITEMS - {len(dfShein)},'+ f' DURACION - {format(end_ - start_)[:-4]},'+\n",
    "           f' CANTIDAD_HOJAS - {paginas},'+ f' FECHA_ALTA - {hoy}' + os.linesep   \n",
    "          )\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dfShein['Tipo'].unique().tolist()) != len(set([item[0] for item in Tlistas])):\n",
    "    print('REVISAR LOS TIPOS SCRAPEADOS')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShein.to_excel(f'../Salida/dfShein_plus{fecha}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36640, 36640, 36640)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfShein.drop_duplicates()), len(dfShein.dropna()), len(dfShein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore = pd.read_excel('plus_size.xlsx')\n",
    "# lista_ = []\n",
    "# for col in restore.columns:\n",
    "#     for i in restore[col]:\n",
    "#         lista_ += [[elementos.strip().replace('\"', '').replace(\"'\", '') for elementos in i.strip('[]').split('], [')[0].split(',')]]\n",
    "        \n",
    "# for index,row in df.iterrows():\n",
    "#     if row['href'] == lista_[-1][-2]:\n",
    "#         print(index+1)\n",
    "#         break\n",
    "# df = df.loc[index+1:].copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev_env]",
   "language": "python",
   "name": "conda-env-dev_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
